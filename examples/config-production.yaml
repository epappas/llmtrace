# ============================================================================
# LLMTrace — Full Production Configuration
# ============================================================================
#
# Complete production setup with ClickHouse + PostgreSQL + Redis, all security
# features enabled, multi-channel alerting, cost controls, and anomaly detection.
#
# Prerequisites:
#   - ClickHouse running on port 8123
#   - PostgreSQL running on port 5432
#   - Redis running on port 6379
#   - (Optional) Binary built with `--features ml` for ML detection
#
# Usage:
#   cp examples/config-production.yaml config.yaml
#   # Edit connection strings and webhook URLs for your environment
#   ./target/release/llmtrace-proxy validate --config config.yaml
#   ./target/release/llmtrace-proxy --config config.yaml
#
# See also:
#   - config-minimal.yaml        — bare minimum for getting started
#   - config-high-security.yaml  — maximum security posture
#   - config-cost-control.yaml   — cost management focus
#   - docs/guides/custom-policies.md — comprehensive configuration guide
# ============================================================================

# --- Network ----------------------------------------------------------------

listen_addr: "0.0.0.0:8080"
upstream_url: "https://api.openai.com"

# --- TLS (recommended for production) --------------------------------------

enable_tls: false
# Uncomment and provide paths to enable TLS:
# enable_tls: true
# tls_cert_file: "/etc/ssl/certs/llmtrace.crt"
# tls_key_file: "/etc/ssl/private/llmtrace.key"

# --- Storage ----------------------------------------------------------------

storage:
  profile: "production"

  # ClickHouse: analytical trace/span storage (high-volume, columnar)
  clickhouse_url: "http://localhost:8123"
  clickhouse_database: "llmtrace"

  # PostgreSQL: metadata, tenant configs, audit events, API keys
  postgres_url: "postgres://llmtrace:llmtrace@localhost:5432/llmtrace"

  # Redis: hot-query cache, session data, rate limit state
  redis_url: "redis://127.0.0.1:6379"

  # In production, run migrations explicitly before deploying:
  #   llmtrace-proxy migrate --config config.yaml
  auto_migrate: false

# --- Logging ----------------------------------------------------------------

logging:
  # Use "info" in production; "debug" only for troubleshooting
  level: "info"
  # JSON format for structured log ingestion (ELK, Datadog, Splunk)
  format: "json"

# --- Timeouts ---------------------------------------------------------------

# 60s timeout for production — some models (o1, Claude Opus) are slow
timeout_ms: 60000
connection_timeout_ms: 10000
max_connections: 5000

# Max request body: 50MB (covers large context windows)
max_request_size_bytes: 52428800

# --- Security Analysis ------------------------------------------------------

enable_security_analysis: true
security_analysis_timeout_ms: 5000

# ML-based detection (requires `ml` feature flag at build time)
security_analysis:
  ml_enabled: false              # Set to true if binary supports it
  ml_model: "protectai/deberta-v3-base-prompt-injection-v2"
  ml_threshold: 0.8
  ml_cache_dir: "/var/lib/llmtrace/models"
  ml_preload: true
  ml_download_timeout_seconds: 300
  ner_enabled: false             # Enable for name/org/location PII detection
  ner_model: "dslim/bert-base-NER"

# --- PII Handling -----------------------------------------------------------

pii:
  # "alert_only" — detect and report, don't modify stored traces
  # "alert_and_redact" — replace PII with [PII:TYPE] tags in stored traces
  action: "alert_only"

# --- Trace Storage ----------------------------------------------------------

enable_trace_storage: true
trace_storage_timeout_ms: 10000

# --- Streaming --------------------------------------------------------------

enable_streaming: true

# Streaming security analysis: check for threats during SSE streams
streaming_analysis:
  enabled: true
  token_interval: 50    # Check every 50 tokens during streaming

# --- Rate Limiting ----------------------------------------------------------

rate_limiting:
  enabled: true
  requests_per_second: 500       # Production-grade throughput
  burst_size: 1000               # 2x sustained for bursty workloads
  window_seconds: 60

  # Per-tenant overrides (add your tenant UUIDs here)
  tenant_overrides: {}
    # "your-high-priority-tenant-uuid":
    #   requests_per_second: 2000
    #   burst_size: 4000
    # "your-dev-tenant-uuid":
    #   requests_per_second: 50
    #   burst_size: 100

# --- Circuit Breaker --------------------------------------------------------

circuit_breaker:
  enabled: true
  failure_threshold: 10          # 10 consecutive failures before opening
  recovery_timeout_ms: 30000     # 30s before probing
  half_open_max_calls: 3

# --- Cost Estimation --------------------------------------------------------

cost_estimation:
  enabled: true
  # External pricing file (hot-reloaded on SIGHUP — no restart needed)
  # pricing_file: "/etc/llmtrace/pricing.yaml"
  custom_models: {}
    # my-fine-tuned-model:
    #   input_per_million: 5.0
    #   output_per_million: 10.0

# --- Cost Caps --------------------------------------------------------------

cost_caps:
  enabled: true

  default_budget_caps:
    - window: hourly
      hard_limit_usd: 50.0      # Hard stop at $50/hour
      soft_limit_usd: 40.0      # Alert at $40/hour

    - window: daily
      hard_limit_usd: 500.0     # Hard stop at $500/day
      soft_limit_usd: 400.0     # Alert at $400/day

  default_token_cap:
    max_prompt_tokens: 16384
    max_completion_tokens: 8192

  agents: []
    # - agent_id: "your-heavy-agent"
    #   budget_caps:
    #     - window: daily
    #       hard_limit_usd: 1000.0
    #       soft_limit_usd: 800.0

# --- Anomaly Detection ------------------------------------------------------

anomaly_detection:
  enabled: true
  window_size: 100               # 100-observation sliding window
  sigma_threshold: 3.0           # 3σ = ~0.3% false positive rate
  check_cost: true
  check_tokens: true
  check_velocity: true
  check_latency: true

# --- Alerting ---------------------------------------------------------------

alerts:
  enabled: true
  cooldown_seconds: 300          # 5-minute deduplication window

  channels:
    # Slack: all Medium+ findings for team awareness
    - type: slack
      url: "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
      min_severity: "Medium"
      min_security_score: 50

    # PagerDuty: Critical only — wake the on-call engineer
    - type: pagerduty
      routing_key: "YOUR_PAGERDUTY_ROUTING_KEY"
      min_severity: "Critical"
      min_security_score: 90

    # SIEM webhook: High+ for security audit trail
    - type: webhook
      url: "https://your-siem.internal/api/llmtrace-alerts"
      min_severity: "High"
      min_security_score: 70

  # Optional: escalate unacknowledged alerts
  # escalation:
  #   enabled: true
  #   escalate_after_seconds: 600

# --- Authentication ---------------------------------------------------------

auth:
  enabled: true
  # Bootstrap admin key — use this for initial tenant/key setup, then rotate
  admin_key: "llmt_change_me_in_production"

# --- gRPC Ingestion ---------------------------------------------------------

grpc:
  enabled: false
  listen_addr: "0.0.0.0:50051"

# --- Health Check -----------------------------------------------------------

health_check:
  enabled: true
  path: "/health"
  interval_seconds: 10
  timeout_ms: 5000
  retries: 3

# --- Graceful Shutdown ------------------------------------------------------

# In Kubernetes, set terminationGracePeriodSeconds > timeout_seconds
# (e.g., 60s grace period with 30s shutdown timeout = 30s headroom)
shutdown:
  timeout_seconds: 30
