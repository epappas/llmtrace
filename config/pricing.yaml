# ============================================================================
# LLMTrace — Model Pricing Configuration
# ============================================================================
#
# Last updated: 2026-02-01
#
# This file defines per-model pricing (cost per 1 million tokens) used by the
# cost estimation engine.  The proxy loads this at startup and reloads it on
# SIGHUP without a rebuild.
#
# If this file is missing or cannot be parsed, built-in defaults are used.
#
# Keys are lowercase model name prefixes — lookup tries both exact match
# and prefix match so that e.g. "gpt-4o-2024-08-06" matches "gpt-4o".
#
# All prices are in USD per 1 000 000 tokens (input / output).
# Prices reflect STANDARD processing tier unless otherwise noted.
#
# For provider-specific keys, the format is:
#   <provider>/<model-name>  (e.g. bedrock/claude-sonnet-4.5)
# ============================================================================


# ===========================================================================
#  SECTION 1: DIRECT API PROVIDERS
# ===========================================================================


# ---------------------------------------------------------------------------
# OpenAI — Flagship / GPT-5 series
# Source: https://platform.openai.com/docs/pricing (Standard tier)
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

gpt-5.2:
  input_per_million: 1.75
  output_per_million: 14.00
  # Cached input: $0.175/M

gpt-5.1:
  input_per_million: 1.25
  output_per_million: 10.00
  # Cached input: $0.125/M

gpt-5:
  input_per_million: 1.25
  output_per_million: 10.00
  # Cached input: $0.125/M

gpt-5-mini:
  input_per_million: 0.25
  output_per_million: 2.00
  # Cached input: $0.025/M

gpt-5-nano:
  input_per_million: 0.05
  output_per_million: 0.40
  # Cached input: $0.005/M

gpt-5.2-pro:
  input_per_million: 21.00
  output_per_million: 168.00

gpt-5-pro:
  input_per_million: 15.00
  output_per_million: 120.00


# ---------------------------------------------------------------------------
# OpenAI — GPT-4.1 series
# Source: https://platform.openai.com/docs/pricing (Standard tier)
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

gpt-4.1:
  input_per_million: 2.00
  output_per_million: 8.00
  # Cached input: $0.50/M

gpt-4.1-mini:
  input_per_million: 0.40
  output_per_million: 1.60
  # Cached input: $0.10/M

gpt-4.1-nano:
  input_per_million: 0.10
  output_per_million: 0.40
  # Cached input: $0.025/M


# ---------------------------------------------------------------------------
# OpenAI — GPT-4o series
# Source: https://platform.openai.com/docs/pricing (Standard tier)
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

gpt-4o:
  input_per_million: 2.50
  output_per_million: 10.00
  # Cached input: $1.25/M

gpt-4o-mini:
  input_per_million: 0.15
  output_per_million: 0.60
  # Cached input: $0.075/M


# ---------------------------------------------------------------------------
# OpenAI — o-series (reasoning models)
# Source: https://platform.openai.com/docs/pricing (Standard tier)
# Verified: 2026-02-01
# Note: Reasoning tokens are billed as output tokens.
# ---------------------------------------------------------------------------

o1:
  input_per_million: 15.00
  output_per_million: 60.00
  # Cached input: $7.50/M

o1-pro:
  input_per_million: 150.00
  output_per_million: 600.00

o3-pro:
  input_per_million: 20.00
  output_per_million: 80.00

o3:
  input_per_million: 2.00
  output_per_million: 8.00
  # Cached input: $0.50/M

o4-mini:
  input_per_million: 1.10
  output_per_million: 4.40
  # Cached input: $0.275/M

o3-mini:
  input_per_million: 1.10
  output_per_million: 4.40
  # Cached input: $0.55/M

o1-mini:
  input_per_million: 1.10
  output_per_million: 4.40
  # Cached input: $0.55/M


# ---------------------------------------------------------------------------
# OpenAI — Legacy / older models
# Source: https://platform.openai.com/docs/pricing (Standard tier)
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

chatgpt-4o-latest:
  input_per_million: 5.00
  output_per_million: 15.00

gpt-4-turbo:
  input_per_million: 10.00
  output_per_million: 30.00

gpt-4:
  input_per_million: 30.00
  output_per_million: 60.00

gpt-4-32k:
  input_per_million: 60.00
  output_per_million: 120.00

gpt-3.5-turbo:
  input_per_million: 0.50
  output_per_million: 1.50


# ---------------------------------------------------------------------------
# OpenAI — Embeddings
# Source: https://platform.openai.com/docs/pricing
# Verified: 2026-02-01
# Note: Embedding models are input-only; output_per_million set to 0.
# ---------------------------------------------------------------------------

text-embedding-3-small:
  input_per_million: 0.02
  output_per_million: 0.00
  # Batch: $0.01/M

text-embedding-3-large:
  input_per_million: 0.13
  output_per_million: 0.00
  # Batch: $0.065/M

text-embedding-ada-002:
  input_per_million: 0.10
  output_per_million: 0.00
  # Batch: $0.05/M


# ---------------------------------------------------------------------------
# Anthropic — Latest models (direct API)
# Source: https://claude.com/pricing
# Verified: 2026-02-01
# Note: Batch processing saves 50%.
# ---------------------------------------------------------------------------

claude-opus-4.5:
  input_per_million: 5.00
  output_per_million: 25.00
  # Cache write: $6.25/M, Cache read: $0.50/M

claude-sonnet-4.5:
  input_per_million: 3.00
  output_per_million: 15.00
  # Prompts >200K tokens: input $6.00/M, output $22.50/M
  # Cache write (≤200K): $3.75/M, Cache read (≤200K): $0.30/M

claude-haiku-4.5:
  input_per_million: 1.00
  output_per_million: 5.00
  # Cache write: $1.25/M, Cache read: $0.10/M


# ---------------------------------------------------------------------------
# Anthropic — Legacy models (direct API)
# Source: https://claude.com/pricing
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

claude-opus-4.1:
  input_per_million: 15.00
  output_per_million: 75.00
  # Cache write: $18.75/M, Cache read: $1.50/M

claude-sonnet-4:
  input_per_million: 3.00
  output_per_million: 15.00
  # Cache write: $3.75/M, Cache read: $0.30/M

claude-opus-4:
  input_per_million: 15.00
  output_per_million: 75.00
  # Cache write: $18.75/M, Cache read: $1.50/M

claude-3-opus:
  input_per_million: 15.00
  output_per_million: 75.00

claude-3-haiku:
  input_per_million: 0.25
  output_per_million: 1.25
  # Cache write: $0.30/M, Cache read: $0.03/M

# Aliases for older naming conventions (dot notation)
claude-3.5-sonnet:
  input_per_million: 3.00
  output_per_million: 15.00

claude-3-5-sonnet:
  input_per_million: 3.00
  output_per_million: 15.00

claude-3.5-haiku:
  input_per_million: 1.00
  output_per_million: 5.00

claude-3-5-haiku:
  input_per_million: 1.00
  output_per_million: 5.00


# ---------------------------------------------------------------------------
# Google — Gemini 3 series (Preview)
# Source: https://cloud.google.com/vertex-ai/generative-ai/pricing
# Verified: 2026-02-01
# Note: Vertex AI pricing. Google AI Studio may differ slightly.
# ---------------------------------------------------------------------------

gemini-3-pro:
  input_per_million: 2.00
  output_per_million: 12.00
  # >200K context: input $4.00/M, output $18.00/M
  # Cached input: $0.20/M (≤200K), $0.40/M (>200K)
  # Batch: 50% discount

gemini-3-flash:
  input_per_million: 0.50
  output_per_million: 3.00
  # Cached input: $0.05/M
  # Batch: 50% discount


# ---------------------------------------------------------------------------
# Google — Gemini 2.5 series
# Source: https://cloud.google.com/vertex-ai/generative-ai/pricing
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

gemini-2.5-pro:
  input_per_million: 1.25
  output_per_million: 10.00
  # >200K context: input $2.50/M, output $15.00/M
  # Cached input: $0.125/M (≤200K), $0.250/M (>200K)
  # Batch: 50% discount

gemini-2.5-flash:
  input_per_million: 0.30
  output_per_million: 2.50
  # Audio input: $1.00/M
  # Cached input: $0.030/M
  # Batch: 50% discount

gemini-2.5-flash-lite:
  input_per_million: 0.10
  output_per_million: 0.40
  # Audio input: $0.30/M
  # Cached input: $0.010/M
  # Batch: 50% discount


# ---------------------------------------------------------------------------
# Google — Gemini 2.0 series
# Source: https://cloud.google.com/vertex-ai/generative-ai/pricing
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

gemini-2.0-flash:
  input_per_million: 0.15
  output_per_million: 0.60
  # Audio input: $1.00/M
  # Batch: 50% discount

gemini-2.0-flash-lite:
  input_per_million: 0.075
  output_per_million: 0.30
  # Batch: 50% discount


# ---------------------------------------------------------------------------
# Google — Gemini 1.5 series
# Source: https://ai.google.dev/gemini-api/docs/pricing (via MetaCTO)
# Verified: 2026-02-01
# Note: Tiered pricing based on prompt length.
# ---------------------------------------------------------------------------

gemini-1.5-pro:
  input_per_million: 1.25
  output_per_million: 5.00
  # >128K context: input $2.50/M, output $10.00/M
  # Cached input (≤128K): $0.3125/M
  # Context cache storage: $4.50/hour

gemini-1.5-flash:
  input_per_million: 0.075
  output_per_million: 0.30
  # >128K context: input $0.15/M, output $0.60/M

gemini-1.5-flash-8b:
  input_per_million: 0.0375
  output_per_million: 0.15
  # >128K context: input $0.075/M, output $0.30/M


# ---------------------------------------------------------------------------
# Google — Embeddings
# Source: https://ai.google.dev/gemini-api/docs/pricing (via MetaCTO)
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

text-embedding-004:
  input_per_million: 0.00
  output_per_million: 0.00
  # Free of charge (Google AI Studio)


# ---------------------------------------------------------------------------
# Mistral — La Plateforme API
# Source: https://mistral.ai/pricing
# Cross-ref: https://pricepertoken.com/pricing-page/provider/mistral-ai
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

mistral-large-2512:
  input_per_million: 0.50
  output_per_million: 1.50

mistral-large:
  input_per_million: 2.00
  output_per_million: 6.00
  # Covers mistral-large-2411 and mistral-large-2407

mistral-medium-3.1:
  input_per_million: 0.40
  output_per_million: 2.00

mistral-medium-3:
  input_per_million: 0.40
  output_per_million: 2.00

mistral-small-3.1:
  input_per_million: 0.03
  output_per_million: 0.11

mistral-small-3.2:
  input_per_million: 0.06
  output_per_million: 0.18

mistral-small-3:
  input_per_million: 0.03
  output_per_million: 0.11

codestral:
  input_per_million: 0.30
  output_per_million: 0.90
  # codestral-2508

devstral-2512:
  input_per_million: 0.05
  output_per_million: 0.22

devstral-medium:
  input_per_million: 0.40
  output_per_million: 2.00

devstral-small:
  input_per_million: 0.10
  output_per_million: 0.30

pixtral-12b:
  input_per_million: 0.10
  output_per_million: 0.10

pixtral-large:
  input_per_million: 2.00
  output_per_million: 6.00
  # pixtral-large-2411

mistral-nemo:
  input_per_million: 0.02
  output_per_million: 0.04

mistral-saba:
  input_per_million: 0.20
  output_per_million: 0.60

ministral-3b:
  input_per_million: 0.04
  output_per_million: 0.04

ministral-8b:
  input_per_million: 0.10
  output_per_million: 0.10

mistral-small-creative:
  input_per_million: 0.10
  output_per_million: 0.30


# ---------------------------------------------------------------------------
# Cohere (direct API)
# Source: https://cohere.com/pricing (via MetaCTO verified Jan 2026)
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

command-r-plus:
  input_per_million: 2.50
  output_per_million: 10.00
  # command-r-plus-08-2024

command-a:
  input_per_million: 2.50
  output_per_million: 10.00

command-r:
  input_per_million: 0.15
  output_per_million: 0.60
  # command-r-08-2024

command-r7b:
  input_per_million: 0.0375
  output_per_million: 0.15

embed-4:
  input_per_million: 0.12
  output_per_million: 0.00
  # Text embedding; image embedding priced separately


# ---------------------------------------------------------------------------
# DeepSeek (direct API)
# Source: https://api-docs.deepseek.com/quick_start/pricing
# Verified: 2026-02-01
# Note: Both models run DeepSeek-V3.2.
#       Cache hit input: $0.028/M (90% discount)
# ---------------------------------------------------------------------------

deepseek-chat:
  input_per_million: 0.28
  output_per_million: 0.42
  # Cache hit input: $0.028/M

deepseek-reasoner:
  input_per_million: 0.28
  output_per_million: 0.42
  # Cache hit input: $0.028/M
  # Thinking mode of DeepSeek-V3.2


# ---------------------------------------------------------------------------
# Perplexity — Sonar API
# Source: https://pricepertoken.com/pricing-page/provider/perplexity
# Verified: 2026-02-01
# Note: All models include web search grounding.
# ---------------------------------------------------------------------------

sonar:
  input_per_million: 1.00
  output_per_million: 1.00

sonar-pro:
  input_per_million: 3.00
  output_per_million: 15.00

sonar-pro-search:
  input_per_million: 3.00
  output_per_million: 15.00

sonar-reasoning-pro:
  input_per_million: 2.00
  output_per_million: 8.00

sonar-deep-research:
  input_per_million: 2.00
  output_per_million: 8.00


# ===========================================================================
#  SECTION 2: HYPERSCALER PROVIDERS
# ===========================================================================


# ---------------------------------------------------------------------------
# Azure OpenAI — Global Standard deployment
# Source: https://azure.microsoft.com/en-us/pricing/details/azure-openai/
# Cross-ref: https://www.finout.io/blog/azure-openai-pricing-6-ways-to-cut-costs
# Verified: 2026-02-01
# Note: Azure OpenAI Global Standard pricing matches OpenAI direct for
#       most models. Regional/Data Zone pricing may differ.
#       Batch API: 50% discount on Global Standard.
# ---------------------------------------------------------------------------

azure/gpt-5:
  input_per_million: 1.25
  output_per_million: 10.00
  # Cached input: $0.13/M

azure/gpt-5-pro:
  input_per_million: 15.00
  output_per_million: 120.00

azure/gpt-5-mini:
  input_per_million: 0.25
  output_per_million: 2.00

azure/gpt-5-nano:
  input_per_million: 0.05
  output_per_million: 0.40

azure/gpt-4.1:
  input_per_million: 2.00
  output_per_million: 8.00
  # Cached input: $0.50/M

azure/gpt-4.1-mini:
  input_per_million: 0.40
  output_per_million: 1.60
  # Cached input: $0.10/M

azure/gpt-4.1-nano:
  input_per_million: 0.10
  output_per_million: 0.40

azure/gpt-4o:
  input_per_million: 2.50
  output_per_million: 10.00
  # Cached input: $1.25/M

azure/gpt-4o-mini:
  input_per_million: 0.15
  output_per_million: 0.60
  # Cached input: $0.075/M

azure/o3:
  input_per_million: 2.00
  output_per_million: 8.00
  # Cached input: $0.50/M

azure/o4-mini:
  input_per_million: 1.10
  output_per_million: 4.40
  # Cached input: $0.275/M

azure/o3-mini:
  input_per_million: 1.10
  output_per_million: 4.40
  # Cached input: $0.55/M

azure/o1:
  input_per_million: 15.00
  output_per_million: 60.00
  # Cached input: $7.50/M


# ---------------------------------------------------------------------------
# AWS Bedrock — Anthropic Claude (US East/Oregon, Standard on-demand)
# Source: https://aws.amazon.com/bedrock/pricing/
# Cross-ref: https://caylent.com/blog/amazon-bedrock-pricing-explained
# Verified: 2026-02-01
# Note: Bedrock prices per 1,000 tokens; converted to per 1M here.
#       Batch pricing is 50% of on-demand. Prices vary by region.
# ---------------------------------------------------------------------------

bedrock/claude-sonnet-4.5:
  input_per_million: 3.00
  output_per_million: 15.00
  # Cache write: $3.75/M, Cache read: $0.30/M

bedrock/claude-haiku-4.5:
  input_per_million: 1.00
  output_per_million: 5.00
  # Cache write: $1.25/M, Cache read: $0.10/M

bedrock/claude-opus-4.1:
  input_per_million: 15.00
  output_per_million: 75.00
  # Cache write: $18.75/M, Cache read: $1.50/M

bedrock/claude-sonnet-4:
  input_per_million: 3.00
  output_per_million: 15.00

bedrock/claude-3.5-sonnet:
  input_per_million: 3.00
  output_per_million: 15.00
  # Extended access pricing (eff. 1 Dec 2025)

bedrock/claude-3-haiku:
  input_per_million: 0.25
  output_per_million: 1.25


# ---------------------------------------------------------------------------
# AWS Bedrock — Meta Llama (US East/Oregon, Standard on-demand)
# Source: https://aws.amazon.com/bedrock/pricing/
# Cross-ref: https://aws.amazon.com/blogs/machine-learning/demystifying-amazon-bedrock-pricing-for-a-chatbot-assistant/
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

bedrock/llama-4-maverick:
  input_per_million: 0.34
  output_per_million: 0.99
  # Bedrock Llama 4 Maverick (17Bx128E)

bedrock/llama-4-scout:
  input_per_million: 0.18
  output_per_million: 0.54
  # Bedrock Llama 4 Scout (17Bx16E)

bedrock/llama-3.3-70b:
  input_per_million: 0.72
  output_per_million: 0.72
  # Llama 3.3 70B Instruct

bedrock/llama-3.1-405b:
  input_per_million: 2.40
  output_per_million: 2.40

bedrock/llama-3.1-70b:
  input_per_million: 0.72
  output_per_million: 0.72

bedrock/llama-3.1-8b:
  input_per_million: 0.22
  output_per_million: 0.22


# ---------------------------------------------------------------------------
# AWS Bedrock — Amazon Nova (US East/Oregon, Standard on-demand)
# Source: https://aws.amazon.com/nova/pricing/
# Cross-ref: https://caylent.com/blog/amazon-bedrock-pricing-explained
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

bedrock/nova-premier:
  input_per_million: 2.50
  output_per_million: 12.50
  # Most capable Amazon model

bedrock/nova-pro:
  input_per_million: 0.80
  output_per_million: 3.20
  # Balanced performance

bedrock/nova-lite:
  input_per_million: 0.06
  output_per_million: 0.24
  # Fast and cost-effective multimodal

bedrock/nova-micro:
  input_per_million: 0.035
  output_per_million: 0.14
  # Text-only, lowest cost
  # Cached input: $0.00875/M


# ---------------------------------------------------------------------------
# AWS Bedrock — Amazon Titan (US East/Oregon, Standard on-demand)
# Source: https://aws.amazon.com/bedrock/pricing/
# Cross-ref: https://caylent.com/blog/amazon-bedrock-pricing-explained
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

bedrock/titan-text-express:
  input_per_million: 0.80
  output_per_million: 1.60

bedrock/titan-text-lite:
  input_per_million: 0.30
  output_per_million: 0.40

bedrock/titan-text-embeddings-v2:
  input_per_million: 0.11
  output_per_million: 0.00
  # Embedding model — input only


# ---------------------------------------------------------------------------
# AWS Bedrock — Mistral (US East/Oregon, Standard on-demand)
# Source: https://aws.amazon.com/bedrock/pricing/
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

bedrock/mistral-large-3:
  input_per_million: 0.50
  output_per_million: 1.50

bedrock/magistral-small:
  input_per_million: 0.50
  output_per_million: 1.50

bedrock/ministral-3b:
  input_per_million: 0.10
  output_per_million: 0.10

bedrock/ministral-8b:
  input_per_million: 0.15
  output_per_million: 0.15

bedrock/ministral-14b:
  input_per_million: 0.20
  output_per_million: 0.20


# ---------------------------------------------------------------------------
# AWS Bedrock — Other providers (US East/Oregon, Standard on-demand)
# Source: https://aws.amazon.com/bedrock/pricing/
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

# DeepSeek on Bedrock
bedrock/deepseek-r1:
  input_per_million: 1.35
  output_per_million: 5.40
  # Pricing from Bedrock DeepSeek section

# Qwen on Bedrock
bedrock/qwen3-next-80b:
  input_per_million: 0.15
  output_per_million: 1.20

bedrock/qwen3-vl-235b:
  input_per_million: 0.53
  output_per_million: 2.66

# Writer on Bedrock
bedrock/palmyra-x4:
  input_per_million: 2.50
  output_per_million: 10.00

bedrock/palmyra-x5:
  input_per_million: 0.60
  output_per_million: 6.00

# Kimi K2 on Bedrock
bedrock/kimi-k2-thinking:
  input_per_million: 0.60
  output_per_million: 2.50

# MiniMax on Bedrock
bedrock/minimax-m2:
  input_per_million: 0.30
  output_per_million: 1.20

# NVIDIA on Bedrock
bedrock/nemotron-nano-2:
  input_per_million: 0.06
  output_per_million: 0.23

# OpenAI GPT-OSS on Bedrock
bedrock/gpt-oss-120b:
  input_per_million: 0.15
  output_per_million: 0.60

bedrock/gpt-oss-20b:
  input_per_million: 0.07
  output_per_million: 0.20

# Google Gemma on Bedrock
bedrock/gemma-3-27b:
  input_per_million: 0.23
  output_per_million: 0.38

bedrock/gemma-3-12b:
  input_per_million: 0.09
  output_per_million: 0.29

bedrock/gemma-3-4b:
  input_per_million: 0.04
  output_per_million: 0.08


# ---------------------------------------------------------------------------
# Google Cloud Vertex AI — Gemini (same models, separate billing)
# Source: https://cloud.google.com/vertex-ai/generative-ai/pricing
# Verified: 2026-02-01
# Note: Vertex AI pricing may differ from Google AI Studio (ai.google.dev).
#       These entries are for users who explicitly route through Vertex AI.
# ---------------------------------------------------------------------------

vertex/gemini-3-pro:
  input_per_million: 2.00
  output_per_million: 12.00

vertex/gemini-3-flash:
  input_per_million: 0.50
  output_per_million: 3.00

vertex/gemini-2.5-pro:
  input_per_million: 1.25
  output_per_million: 10.00

vertex/gemini-2.5-flash:
  input_per_million: 0.30
  output_per_million: 2.50

vertex/gemini-2.5-flash-lite:
  input_per_million: 0.10
  output_per_million: 0.40

vertex/gemini-2.0-flash:
  input_per_million: 0.15
  output_per_million: 0.60

vertex/gemini-2.0-flash-lite:
  input_per_million: 0.075
  output_per_million: 0.30


# ---------------------------------------------------------------------------
# IBM watsonx.ai — Granite and hosted models
# Source: https://www.ibm.com/products/watsonx-ai/pricing
# Cross-ref: https://ibmlicensingexperts.com/ibm-watsonx-licensing-pricing-getting-value-from-ibms-ai-platform/
# Verified: 2026-02-01
# Note: IBM watsonx.ai uses Resource Units (1 RU = 1,000 tokens).
#       Pricing is NOT split into input/output — charged per total tokens.
#       IBM does not publicly list exact per-model token prices on their
#       website; prices below are approximate based on third-party reports.
#       Granite models are first-party (lower cost); third-party models
#       like Llama are hosted at higher rates.
#       Contact IBM for exact pricing for your plan.
# ---------------------------------------------------------------------------

# watsonx/granite:
#   input_per_million: ~100.00
#   output_per_million: ~100.00
#   # NOTE: IBM charges ~$0.10 per 1,000 tokens (both input+output combined)
#   # which is ~$100/M total tokens. This is significantly more expensive
#   # than direct API providers. Pricing depends on plan tier and volume.
#   # IBM does not publish granular per-model pricing publicly.
#   # Omitted from active pricing due to unverifiable exact rates.


# ---------------------------------------------------------------------------
# Oracle OCI Generative AI
# Source: https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing/
# Verified: 2026-02-01
# Note: OCI prices by "transactions" (1 transaction = 1 character).
#       OCI also offers xAI Grok, Google Gemini, and OpenAI GPT-OSS models.
#       Exact per-token prices require login to Oracle Cloud console.
#       OCI's pricing model (per character) makes direct comparison complex.
#       Entries below are for xAI Grok models where token-based pricing
#       is listed on the Oracle pricing page.
# ---------------------------------------------------------------------------

# Oracle OCI does not publicly list exact USD/token prices in their
# pricing page tables (requires sign-in). Models available include:
#   - Meta Llama 4 Scout/Maverick, Llama 3.1/3.2
#   - Cohere Command R+/R, Embed
#   - xAI Grok 3/4 (token-based pricing)
#   - Google Gemini 2.5 Pro/Flash (token-based pricing)
#   - OpenAI gpt-oss-120b/20b (token-based pricing)
# Contact Oracle for exact rates or check console.


# ===========================================================================
#  SECTION 3: HOSTED INFERENCE PROVIDERS
# ===========================================================================


# ---------------------------------------------------------------------------
# Together AI — Serverless inference
# Source: https://www.together.ai/pricing
# Verified: 2026-02-01
# ---------------------------------------------------------------------------

together/llama-4-maverick:
  input_per_million: 0.27
  output_per_million: 0.85

together/llama-4-scout:
  input_per_million: 0.18
  output_per_million: 0.59

together/llama-3.3-70b:
  input_per_million: 0.88
  output_per_million: 0.88

together/llama-3.1-405b:
  input_per_million: 3.50
  output_per_million: 3.50

together/llama-3.1-70b:
  input_per_million: 0.88
  output_per_million: 0.88

together/llama-3.1-8b:
  input_per_million: 0.18
  output_per_million: 0.18

together/llama-3.2-3b:
  input_per_million: 0.06
  output_per_million: 0.06

together/deepseek-r1-0528:
  input_per_million: 3.00
  output_per_million: 7.00

together/deepseek-v3.1:
  input_per_million: 0.60
  output_per_million: 1.25

together/deepseek-v3-0324:
  input_per_million: 1.25
  output_per_million: 1.25

together/qwen3-235b:
  input_per_million: 0.65
  output_per_million: 3.00

together/qwen2.5-72b:
  input_per_million: 1.20
  output_per_million: 1.20

together/mistral-small-3:
  input_per_million: 0.80
  output_per_million: 0.80

together/gpt-oss-120b:
  input_per_million: 0.15
  output_per_million: 0.60

together/gpt-oss-20b:
  input_per_million: 0.05
  output_per_million: 0.20

together/kimi-k2.5:
  input_per_million: 0.50
  output_per_million: 2.80

together/kimi-k2-instruct:
  input_per_million: 1.00
  output_per_million: 3.00

together/glm-4.7:
  input_per_million: 0.45
  output_per_million: 2.00


# ---------------------------------------------------------------------------
# Fireworks AI — Serverless inference
# Source: https://fireworks.ai/pricing
# Verified: 2026-02-01
# Note: Base model pricing is by parameter count tier.
#       Specific model pricing listed where available.
# ---------------------------------------------------------------------------

fireworks/deepseek-v3:
  input_per_million: 0.56
  output_per_million: 1.68
  # DeepSeek V3 family

fireworks/deepseek-r1-0528:
  input_per_million: 1.35
  output_per_million: 5.40

fireworks/qwen3-235b:
  input_per_million: 0.22
  output_per_million: 0.88

fireworks/kimi-k2-instruct:
  input_per_million: 0.60
  output_per_million: 2.50

fireworks/kimi-k2.5:
  input_per_million: 0.60
  output_per_million: 3.00
  # Cached input: $0.10/M

fireworks/glm-4.7:
  input_per_million: 0.60
  output_per_million: 2.20

fireworks/gpt-oss-120b:
  input_per_million: 0.15
  output_per_million: 0.60

fireworks/gpt-oss-20b:
  input_per_million: 0.07
  output_per_million: 0.30

# Fireworks tier-based pricing (generic, for models not listed above):
# <4B params:  $0.10/M input+output
# 4-16B:       $0.20/M input+output
# >16B:        $0.90/M input+output
# MoE 0-56B:   $0.50/M input+output
# MoE 56-176B: $1.20/M input+output


# ---------------------------------------------------------------------------
# Groq — LPU inference (ultra-low latency)
# Source: https://groq.com/pricing
# Verified: 2026-02-01
# Note: Groq offers batch API at 50% discount.
#       Cached input at 50% discount where available.
# ---------------------------------------------------------------------------

groq/llama-4-scout:
  input_per_million: 0.11
  output_per_million: 0.34

groq/llama-4-maverick:
  input_per_million: 0.20
  output_per_million: 0.60

groq/llama-3.3-70b:
  input_per_million: 0.59
  output_per_million: 0.79

groq/llama-3.1-8b:
  input_per_million: 0.05
  output_per_million: 0.08

groq/qwen3-32b:
  input_per_million: 0.29
  output_per_million: 0.59

groq/gpt-oss-120b:
  input_per_million: 0.15
  output_per_million: 0.60
  # Cached input: $0.075/M

groq/gpt-oss-20b:
  input_per_million: 0.075
  output_per_million: 0.30
  # Cached input: $0.0375/M

groq/kimi-k2:
  input_per_million: 1.00
  output_per_million: 3.00
  # Cached input: $0.50/M


# ===========================================================================
#  SECTION 4: CONVENIENCE ALIASES (unprefixed Llama models)
# ===========================================================================
# These unprefixed entries use Together AI pricing as a reasonable default
# for users who don't specify a provider prefix.
# ---------------------------------------------------------------------------

llama-4-maverick:
  input_per_million: 0.27
  output_per_million: 0.85

llama-4-scout:
  input_per_million: 0.18
  output_per_million: 0.59

llama-3.3-70b:
  input_per_million: 0.88
  output_per_million: 0.88

llama-3.1-405b:
  input_per_million: 3.50
  output_per_million: 3.50

llama-3.1-70b:
  input_per_million: 0.88
  output_per_million: 0.88

llama-3.1-8b:
  input_per_million: 0.18
  output_per_million: 0.18

llama-3.2-3b:
  input_per_million: 0.06
  output_per_million: 0.06

deepseek-r1:
  input_per_million: 3.00
  output_per_million: 7.00

deepseek-v3.1:
  input_per_million: 0.60
  output_per_million: 1.25
