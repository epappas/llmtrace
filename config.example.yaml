# ============================================================================
# LLMTrace Proxy — Example Configuration
# ============================================================================
#
# Copy this file to config.yaml and adjust values for your environment.
#
# Usage:
#   llmtrace-proxy --config config.yaml          # start the proxy
#   llmtrace-proxy validate --config config.yaml  # validate without starting
#   llmtrace-proxy --help                          # show CLI help
#   llmtrace-proxy --version                       # show version
#
# Environment variable overrides (highest precedence after CLI flags):
#   LLMTRACE_CONFIG                — path to this config file
#   LLMTRACE_LISTEN_ADDR           — override listen_addr
#   LLMTRACE_UPSTREAM_URL          — override upstream_url
#   LLMTRACE_STORAGE_PROFILE       — override storage.profile
#   LLMTRACE_STORAGE_DATABASE_PATH — override storage.database_path
#   LLMTRACE_CLICKHOUSE_URL        — override storage.clickhouse_url
#   LLMTRACE_CLICKHOUSE_DATABASE   — override storage.clickhouse_database
#   LLMTRACE_POSTGRES_URL          — override storage.postgres_url
#   LLMTRACE_REDIS_URL             — override storage.redis_url
#   LLMTRACE_LOG_LEVEL             — override logging.level
#   LLMTRACE_LOG_FORMAT            — override logging.format
#   RUST_LOG                       — fine-grained tracing filter (e.g. "llmtrace_proxy=debug,info")
# ============================================================================

# ---------------------------------------------------------------------------
# Network
# ---------------------------------------------------------------------------

# Address and port to bind the proxy server to.
listen_addr: "0.0.0.0:8080"

# Upstream LLM provider URL (OpenAI, vLLM, Ollama, etc.).
upstream_url: "https://api.openai.com"

# ---------------------------------------------------------------------------
# Storage
# ---------------------------------------------------------------------------

storage:
  # Profile: "lite" (SQLite, zero infrastructure), "memory" (in-memory, lost on restart),
  # or "production" (ClickHouse + PostgreSQL + Redis — see docker-compose.yml).
  profile: "lite"
  # Database file path (used by the "lite" profile).
  database_path: "llmtrace.db"

  # --- Production profile settings (only used when profile = "production") ---
  # ClickHouse HTTP URL for trace/span analytical storage.
  # clickhouse_url: "http://localhost:8123"
  # ClickHouse database name (created automatically on startup).
  # clickhouse_database: "llmtrace"
  # PostgreSQL connection URL for metadata (tenants, configs, audit events).
  # postgres_url: "postgres://llmtrace:llmtrace@localhost:5432/llmtrace"
  # Redis connection URL for hot-query cache and sessions.
  # redis_url: "redis://127.0.0.1:6379"

  # Automatically run pending database migrations on startup.
  # Default: true (convenient for development). Set to false in production
  # and use `llmtrace-proxy migrate --config config.yaml` explicitly.
  auto_migrate: true

# ---------------------------------------------------------------------------
# Logging
# ---------------------------------------------------------------------------

logging:
  # Log level: trace, debug, info, warn, error.
  # Can be overridden by LLMTRACE_LOG_LEVEL env var or --log-level CLI flag.
  # RUST_LOG env var takes highest precedence for fine-grained control.
  level: "info"
  # Output format: "text" (human-readable) or "json" (structured, machine-parseable).
  # Can be overridden by LLMTRACE_LOG_FORMAT env var or --log-format CLI flag.
  format: "text"

# ---------------------------------------------------------------------------
# Timeouts
# ---------------------------------------------------------------------------

# Request timeout in milliseconds (covers full upstream round-trip).
timeout_ms: 30000

# Connection timeout in milliseconds (TCP connect phase only).
connection_timeout_ms: 5000

# Maximum number of concurrent connections.
max_connections: 1000

# ---------------------------------------------------------------------------
# TLS (for the proxy listener itself)
# ---------------------------------------------------------------------------

enable_tls: false
# tls_cert_file: "/etc/ssl/certs/proxy.crt"
# tls_key_file: "/etc/ssl/private/proxy.key"

# ---------------------------------------------------------------------------
# Feature toggles
# ---------------------------------------------------------------------------

# Enable regex-based security analysis (prompt injection, PII detection).
enable_security_analysis: true

# Enable trace storage to the configured storage backend.
enable_trace_storage: true

# Enable streaming SSE passthrough for "stream": true requests.
enable_streaming: true

# Maximum request body size in bytes (default: 50 MB).
max_request_size_bytes: 52428800

# Timeout for async security analysis in milliseconds.
security_analysis_timeout_ms: 5000

# Timeout for async trace storage in milliseconds.
trace_storage_timeout_ms: 10000

# ---------------------------------------------------------------------------
# Rate limiting
# ---------------------------------------------------------------------------

rate_limiting:
  enabled: true
  requests_per_second: 100
  burst_size: 200
  window_seconds: 60
  # Per-tenant overrides — keyed by tenant UUID
  tenant_overrides:
    # "tenant-uuid-here":
    #   requests_per_second: 500
    #   burst_size: 1000

# ---------------------------------------------------------------------------
# Circuit breaker — degrades to pure pass-through on repeated failures
# ---------------------------------------------------------------------------

circuit_breaker:
  enabled: true
  # Number of consecutive failures before opening the circuit.
  failure_threshold: 10
  # Time in ms to wait before trying a probe call (half-open state).
  recovery_timeout_ms: 30000
  # Number of probe calls allowed in half-open state.
  half_open_max_calls: 3

# ---------------------------------------------------------------------------
# Alert engine — webhook notifications for security findings
# ---------------------------------------------------------------------------

alerts:
  # Enable the alert engine. When enabled, security findings that exceed the
  # configured thresholds will trigger notifications to the configured channels.
  enabled: false

  # ---------- Legacy mode (backward compatible) ----------
  # If no `channels` are defined, these top-level fields are used:
  # webhook_url: "https://hooks.slack.com/services/T00/B00/xxx"
  # min_severity: "High"
  # min_security_score: 70

  # ---------- Multi-channel mode ----------
  # When `channels` is defined, each channel has its own type, URL, and severity filter.
  # Supported types: webhook, slack, pagerduty (email: coming soon)
  channels: []
  #   - type: slack
  #     url: "https://hooks.slack.com/services/T00/B00/xxx"
  #     min_severity: "Medium"          # Slack gets Medium and above
  #     min_security_score: 50
  #
  #   - type: pagerduty
  #     routing_key: "your-pagerduty-routing-key"
  #     min_severity: "Critical"        # PagerDuty only for Critical
  #     min_security_score: 90
  #
  #   - type: webhook
  #     url: "https://your-server.com/alerts"
  #     min_severity: "High"
  #     min_security_score: 70

  # Global cooldown in seconds between repeated alerts for the same finding type.
  cooldown_seconds: 300

  # Optional: alert escalation (re-send at higher severity if unacknowledged).
  # escalation:
  #   enabled: false
  #   escalate_after_seconds: 600

# ---------------------------------------------------------------------------
# Cost estimation — model pricing for per-request cost tracking
# ---------------------------------------------------------------------------

cost_estimation:
  # Enable cost estimation on traced requests.
  enabled: true

  # Optional: external pricing file. When set, the proxy loads model pricing
  # from this YAML file at startup and reloads it on SIGHUP — no rebuild needed.
  # If the file is missing or invalid, built-in defaults are used.
  # pricing_file: "config/pricing.yaml"

  # Inline custom model pricing overrides (take highest precedence).
  # custom_models:
  #   my-fine-tuned-model:
  #     input_per_million: 5.0
  #     output_per_million: 10.0

# ---------------------------------------------------------------------------
# Cost caps — per-agent budget & token enforcement
# ---------------------------------------------------------------------------

cost_caps:
  # Enable cost cap enforcement. When enabled, requests are checked against
  # budget limits (USD) and per-request token caps before being forwarded.
  enabled: false

  # Default budget caps applied to all tenants/agents unless overridden.
  # Each cap specifies a time window and a hard limit. Requests that would
  # push spend above the hard limit are rejected with HTTP 429.
  # Soft limits trigger an alert but still allow the request through.
  default_budget_caps:
    - window: hourly
      hard_limit_usd: 10.0
      soft_limit_usd: 8.0
    - window: daily
      hard_limit_usd: 100.0
      # soft_limit_usd: 80.0

  # Default per-request token caps (applied before the request is forwarded).
  # default_token_cap:
  #   max_prompt_tokens: 8192
  #   max_completion_tokens: 4096
  #   max_total_tokens: 16384

  # Per-agent overrides. An agent is identified by the X-LLMTrace-Agent-ID header.
  # agents:
  #   - agent_id: "heavy-agent"
  #     budget_caps:
  #       - window: daily
  #         hard_limit_usd: 500.0
  #         soft_limit_usd: 400.0
  #     token_cap:
  #       max_prompt_tokens: 16384
  #       max_completion_tokens: 8192

# ---------------------------------------------------------------------------
# gRPC ingestion gateway — high-throughput trace ingestion via tonic
# ---------------------------------------------------------------------------

grpc:
  # Enable the gRPC ingestion endpoint. When enabled, the proxy starts a
  # tonic gRPC server on a separate listen address that accepts traces in
  # the LLMTrace-native protobuf format (see proto/llmtrace.proto).
  # Supports both unary batch and client-side streaming RPCs.
  enabled: false
  # Address and port to bind the gRPC server to.
  listen_addr: "0.0.0.0:50051"

# ---------------------------------------------------------------------------
# Streaming security analysis — real-time analysis during SSE streaming
# ---------------------------------------------------------------------------

streaming_analysis:
  # Enable incremental regex-based security checks during SSE streaming.
  # When enabled, the proxy runs lightweight pattern matching every N tokens
  # while the stream is still in progress. Findings are tagged with
  # "detection": "streaming" metadata and critical issues trigger alerts
  # mid-stream rather than waiting for stream completion.
  enabled: false
  # Number of completion tokens between each incremental analysis check.
  # Lower values detect threats faster but add marginal CPU overhead per chunk.
  token_interval: 50

# ---------------------------------------------------------------------------
# ML-based security analysis
# ---------------------------------------------------------------------------

# security_analysis:
#   # Enable ML-based prompt injection detection (requires binary compiled with `ml` feature).
#   ml_enabled: false
#   # HuggingFace model ID for prompt injection detection.
#   ml_model: "protectai/deberta-v3-base-prompt-injection-v2"
#   # Confidence threshold for ML detection (0.0–1.0).
#   ml_threshold: 0.8
#   # Local cache directory for downloaded ML models.
#   ml_cache_dir: "~/.cache/llmtrace/models"
#   # Pre-load ML models at proxy startup (eliminates cold-start latency on first request).
#   ml_preload: true
#   # Timeout in seconds for downloading ML models at startup.
#   ml_download_timeout_seconds: 300
#   # Enable ML-based NER for PII detection (person names, organisations, locations).
#   ner_enabled: false
#   # HuggingFace model ID for NER-based PII detection.
#   ner_model: "dslim/bert-base-NER"

# ---------------------------------------------------------------------------
# Graceful shutdown — connection draining and task completion
# ---------------------------------------------------------------------------

shutdown:
  # Maximum seconds to wait for in-flight background tasks (trace capture,
  # security analysis) to complete after a SIGTERM/SIGINT signal. If tasks
  # are still running after this timeout the process force-exits.
  # In Kubernetes, set terminationGracePeriodSeconds > timeout_seconds
  # (e.g. 60s grace period with 30s shutdown timeout gives 30s headroom).
  timeout_seconds: 30

# ---------------------------------------------------------------------------
# Health check endpoint
# ---------------------------------------------------------------------------

health_check:
  enabled: true
  path: "/health"
  interval_seconds: 10
  timeout_ms: 5000
  retries: 3
